{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "OkqiMVfJfjU6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ty9UvziHn2L",
        "outputId": "87c4b90e-c1dd-4625-fed8-f19a5a73ef13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "x7BdViRsSDRd"
      },
      "outputs": [],
      "source": [
        "# Noisy text tile file path\n",
        "file_path = '/content/drive/MyDrive/Lab 2/NoisyText.txt'\n",
        "\n",
        "# video links file path\n",
        "movie_name_path = \"/content/drive/MyDrive/Lab 2/vdoLinks.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R3SpjP6SmtO",
        "outputId": "77420754-16d1-487c-f168-1daaa197ea91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NewMovieDrPQRd\n",
            "K26_sDKnvMU\n",
            "K26_sDKnvMU\n",
            "<HttpError 403 when requesting https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&maxResults=100&videoId=K26_sDKnvMU&textFormat=plainText&key=AIzaSyAEoOSkG60fa3h09UdNPV0rIcavVMHj3tQ&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\">\n",
            "NewMovieDrPQRd\n",
            "3LPANjHlPxo\n",
            "3LPANjHlPxo\n",
            "'charmap' codec can't encode characters in position 33-41: character maps to <undefined>\n",
            "NewMovieDrPQRd\n",
            "rEnOoWs3FuA\n",
            "I was looking for halloween themed movies and stumbled over this... is there anything halloween related in this film?\n",
            "\n",
            "\n",
            "Seen it years ago but can't remember squat\n",
            "welp, been 3 years since anybody have commented\n",
            "Well, there was supposed to be another sequel. From what I heard, the two guys go to Italy (to meet their new relatives, I suppose) & wind up meeting Italian versions of themselves.\n",
            "i loved the first one is it like the first one or not?\n"
          ]
        }
      ],
      "source": [
        "# Reading the file\n",
        "with open(file_path, 'r', encoding= \"latin1\") as file:\n",
        "    raw_text = file.read()\n",
        "    print(raw_text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf5R1MBXHpYt"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_data = pd.read_csv(movie_name_path)\n",
        "print(movie_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9srWh15Vhp1L",
        "outputId": "532eda8c-fe82-4db1-aacf-243265da2a22"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         youtubeId  movieId                                              title\n",
            "0      K26_sDKnvMU        1                                   Toy Story (1995)\n",
            "1      3LPANjHlPxo        2                                     Jumanji (1995)\n",
            "2      rEnOoWs3FuA        3                            Grumpier Old Men (1995)\n",
            "3      j9xml1CxgXI        4                           Waiting to Exhale (1995)\n",
            "4      ltwvKLnj1B4        5                 Father of the Bride Part II (1995)\n",
            "...            ...      ...                                                ...\n",
            "25618  -oB6DN5dYWo   131252  Forklift Driver Klaus: The First Day on the Jo...\n",
            "25619  DK7KQ-gEdl4   131256                      Feuer, Eis & Dosenbier (2002)\n",
            "25620  v29P-wchMZQ   131258                                 The Pirates (2014)\n",
            "25621  dAz-nZ65jYU   131260                                Rentun Ruusu (2001)\n",
            "25622  YWmbl_7VVYk   131262                                   Innocence (2014)\n",
            "\n",
            "[25623 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define slang words to replace\n",
        "slang_words = [\"damn\", \"hell\", \"crap\", \"suck\", \"fuck\", \"shit\", \"bitch\", \"dick\"]\n",
        "\n",
        "def replace_slang(text, slang_list):\n",
        "    \"\"\"Replace slang words in the text with (**).\"\"\"\n",
        "    for slang in slang_list:\n",
        "        pattern = re.compile(re.escape(slang), re.IGNORECASE)  # Case-insensitive replacement\n",
        "        text = pattern.sub(\"(**)\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "lW73Z-Da0-r0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex to identify YouTube-like IDs (11-character alphanumeric, underscores, or hyphens)\n",
        "youtube_id_pattern = re.compile(r\"^[a-zA-Z0-9_-]{11}$\", re.MULTILINE)"
      ],
      "metadata": {
        "id": "es-qNFLQh72E"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the raw text into lines\n",
        "lines = raw_text.splitlines()"
      ],
      "metadata": {
        "id": "uLPIR2-er2cT"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the raw text into lines\n",
        "lines = raw_text.splitlines()"
      ],
      "metadata": {
        "id": "hLnH17J4s1JI"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables\n",
        "parsed_data = []  # To store the cleaned data\n",
        "current_id = None  # To store the current YouTube ID\n",
        "current_comments = []  # To store comments for the current YouTube ID\n",
        "\n",
        "\n",
        "# Process each line in the text\n",
        "for line in lines:\n",
        "    line = line.strip()  # Remove extra spaces\n",
        "    if youtube_id_pattern.match(line):  # Check if the line is a YouTube-like ID\n",
        "        # If we already have an ID, save its data before moving to the next one\n",
        "        if current_id and current_comments:\n",
        "            clean_comments = \"\\n\".join(current_comments).strip()\n",
        "            clean_comments = replace_slang(clean_comments, slang_words)  # Replace slang words\n",
        "            parsed_data.append({\"YouTubeID\": current_id, \"Comments\": clean_comments})\n",
        "        # Update to the new ID\n",
        "        current_id = line\n",
        "        current_comments = []  # Reset comments for the new ID\n",
        "    elif (\n",
        "        \"HttpError\" not in line\n",
        "        and \"codec\" not in line\n",
        "        and \"returned\" not in line\n",
        "        and \"NewMovieDrPQRd\" not in line\n",
        "        and line  # Exclude noise lines and empty lines\n",
        "    ):\n",
        "        current_comments.append(line)  # Append valid comments\n",
        "\n",
        "# After the loop, save the last ID and its comments\n",
        "if current_id and current_comments:\n",
        "    clean_comments = \"\\n\".join(current_comments).strip()\n",
        "    clean_comments = replace_slang(clean_comments, slang_words)  # Replace slang words\n",
        "    parsed_data.append({\"YouTubeID\": current_id, \"Comments\": clean_comments})\n",
        "\n",
        "# Convert parsed data to a DataFrame\n",
        "comments_df = pd.DataFrame(parsed_data)\n",
        "\n",
        "# Ensure consistent column names between DataFrames\n",
        "comments_df.rename(columns={'YouTubeID': 'youtubeId'}, inplace=True)\n",
        "\n",
        "# Merge with the movie titles from the CSV\n",
        "merged_df = pd.merge(movie_data, comments_df, on='youtubeId', how='left')\n",
        "\n",
        "# Fill missing comments with \"No comments were found\"\n",
        "merged_df['Comments'] = merged_df['Comments'].fillna(\"No comments were found\")\n",
        "\n",
        "# Save the cleaned data to Google Drive\n",
        "output_path = \"/content/drive/MyDrive/Lab 2/cleaned_youtube_data.txt\"\n",
        "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
        "    for _, row in merged_df.iterrows():\n",
        "        # Write the movie name\n",
        "        output_file.write(f\"Movie Name: {row['title']}\\n\")\n",
        "\n",
        "        if row['Comments'] == \"No comments were found\":\n",
        "            output_file.write(\"No comments were found\\n\\n\")\n",
        "        else:\n",
        "            output_file.write(\"The Comments are:\\n\")\n",
        "            output_file.write(f\"{row['Comments']}\\n\\n\")\n",
        "\n",
        "print(f\"Cleaned data saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QekQyO_s4nn",
        "outputId": "f1daf3f0-b2d5-48b4-b288-b5037abca532"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to: /content/drive/MyDrive/Lab 2/cleaned_youtube_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of letters before cleaning\n",
        "letter_before = sum(c.isalpha() for c in raw_text)"
      ],
      "metadata": {
        "id": "3CGZUAmlvQhK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine cleaned comments to calculate LETTERSafter\n",
        "cleaned_text = \"\\n\".join(merged_df['Comments'])\n",
        "letter_after = sum(c.isalpha() for c in cleaned_text)\n"
      ],
      "metadata": {
        "id": "LuqHmFFA27hE"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ratio\n",
        "ratio = letter_after / letter_before\n",
        "print(f\"Noise Cleaning Efficiency Ratio (letter after / letter before) is: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysUZ7m_h3Bb2",
        "outputId": "b69c2702-a645-4246-dfa6-513b7a9fb8b9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise Cleaning Efficiency Ratio (letter after / letter before) is: 0.2393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the part of new data\n",
        "cleaned_data_path = \"/content/drive/MyDrive/Lab 2/cleaned_youtube_data.txt\"\n",
        "with open(cleaned_data_path, \"r\") as new_file:\n",
        "  new_data = new_file.read()\n",
        "  print(new_data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9-kSR3fB2BR",
        "outputId": "33c8d982-e352-4791-c60c-2444d1cf22ab"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie Name: Toy Story (1995)\n",
            "No comments were found\n",
            "\n",
            "Movie Name: Jumanji (1995)\n",
            "No comments were found\n",
            "\n",
            "Movie Name: Grumpier Old Men (1995)\n",
            "The Comments are:\n",
            "I was looking for halloween themed movies and stumbled over this... is there anything halloween related in this film?\n",
            "Seen it years ago but can't remember squat\n",
            "welp, been 3 years since anybody have commented\n",
            "Well, there was supposed to be another sequel. From what I heard, the two guys go to Italy (to meet their new relatives, I suppose) & wind up meeting Italian versions of themselves.\n",
            "i loved the first one is it like the first one or not??\n",
            "128238 170 7 0 7\n",
            "The more things change, the more they stay the same in Wabasha, Minnesota. The uncatchable fish named Catfish Hunter grows fatter. The wisecracks, zingers and put downs pile up like freshly raked leaves. And GRUMPY OLD MEN become grumpier in the sequel that's \"pure delight, a wonderfully warmhearted comedy\" (David Sheehan, CBS-TV). Grabbing fishing rods and picking up where t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiMb4UdaCjPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
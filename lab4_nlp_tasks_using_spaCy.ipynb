{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc25fa7-6fb3-4f5f-ad3e-a1c9044cdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d72e5e-af21-468d-8b8c-42e341f94153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the language model \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25ed7b0-69cb-4af4-9510-f056a00ea13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts for each task\n",
    "text_lem = \"The cars are being driven carefully while the dogs are barking.\"\n",
    "text_tok = \"I am learning natural language processing using spaCy.\"\n",
    "text_ner = \"Barack Obama served as the 44th president of the United States and was born in Hawaii.\"\n",
    "text_sent = \"Machine learning is fascinating. It enables computers to learn from data. Natural language processing is a subfield of AI.\"\n",
    "text_stop = \"Stop words are common words that often do not add much meaning.\"\n",
    "text_pos = \"Natural language processing is an exciting field of artificial intelligence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd08e794-fac5-465f-9338-1eb59ea903be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['the', 'car', 'be', 'be', 'drive', 'carefully', 'while', 'the', 'dog', 'be', 'bark', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "print(\"Lemmatization:\", lemmatization(text_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52174bcd-84f6-48e2-a4c9-883ac1ef96a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['I', 'am', 'learning', 'natural', 'language', 'processing', 'using', 'spaCy', '.']\n",
      "Number of Tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "def tokenization(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "print(\"Tokenization:\", tokenization(text_tok))\n",
    "print(\"Number of Tokens:\", len(tokenization(text_tok)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2224f93-acb4-4e47-b269-4630c2e335a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities: [('Barack Obama', 'PERSON'), ('44th', 'ORDINAL'), ('the United States', 'GPE'), ('Hawaii', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "print(\"Named Entities:\", named_entity_recognition(text_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0761cae-8031-4c4c-aff4-e341ccaa5cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Machine learning is fascinating.', 'It enables computers to learn from data.', 'Natural language processing is a subfield of AI.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Segmentation\n",
    "def sentence_segmentation(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "print(\"Sentences:\", sentence_segmentation(text_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5648462-bc03-4820-9f41-3b4cec7489bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Stop Words: ['Stop', 'words', 'common', 'words', 'add', 'meaning', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Word Removal\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"Without Stop Words:\", remove_stopwords(text_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c1f6581-a83d-4dfb-88b4-4fb729977ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('Natural', 'ADJ', 'adjective'), ('language', 'NOUN', 'noun'), ('processing', 'NOUN', 'noun'), ('is', 'AUX', 'auxiliary'), ('an', 'DET', 'determiner'), ('exciting', 'ADJ', 'adjective'), ('field', 'NOUN', 'noun'), ('of', 'ADP', 'adposition'), ('artificial', 'ADJ', 'adjective'), ('intelligence', 'NOUN', 'noun'), ('.', 'PUNCT', 'punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# Parts of Speech (POS) Tagging\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_, spacy.explain(token.pos_)) for token in doc]\n",
    "\n",
    "print(\"POS Tagging:\", pos_tagging(text_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7ddcfbd-bb5b-4eed-878e-ea4fb7b49600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full NLP Pipeline Output:\n",
      "Sentences: ['Albert Einstein, a theoretical physicist, was born in Germany.', 'He developed the theory of relativity, which is one of the two pillars of modern physics.']\n",
      "Tokens: ['Albert', 'Einstein', ',', 'a', 'theoretical', 'physicist', ',', 'was', 'born', 'in', 'Germany', '.', 'He', 'developed', 'the', 'theory', 'of', 'relativity', ',', 'which', 'is', 'one', 'of', 'the', 'two', 'pillars', 'of', 'modern', 'physics', '.']\n",
      "Without Stop Words: ['Albert', 'Einstein', ',', 'theoretical', 'physicist', ',', 'born', 'Germany', '.', 'developed', 'theory', 'relativity', ',', 'pillars', 'modern', 'physics', '.']\n",
      "Lemmas: ['Albert', 'Einstein', ',', 'a', 'theoretical', 'physicist', ',', 'be', 'bear', 'in', 'Germany', '.', 'he', 'develop', 'the', 'theory', 'of', 'relativity', ',', 'which', 'be', 'one', 'of', 'the', 'two', 'pillar', 'of', 'modern', 'physic', '.']\n",
      "POS Tags: [('Albert', 'PROPN', 'proper noun'), ('Einstein', 'PROPN', 'proper noun'), (',', 'PUNCT', 'punctuation'), ('a', 'DET', 'determiner'), ('theoretical', 'ADJ', 'adjective'), ('physicist', 'NOUN', 'noun'), (',', 'PUNCT', 'punctuation'), ('was', 'AUX', 'auxiliary'), ('born', 'VERB', 'verb'), ('in', 'ADP', 'adposition'), ('Germany', 'PROPN', 'proper noun'), ('.', 'PUNCT', 'punctuation'), ('He', 'PRON', 'pronoun'), ('developed', 'VERB', 'verb'), ('the', 'DET', 'determiner'), ('theory', 'NOUN', 'noun'), ('of', 'ADP', 'adposition'), ('relativity', 'NOUN', 'noun'), (',', 'PUNCT', 'punctuation'), ('which', 'PRON', 'pronoun'), ('is', 'AUX', 'auxiliary'), ('one', 'NUM', 'numeral'), ('of', 'ADP', 'adposition'), ('the', 'DET', 'determiner'), ('two', 'NUM', 'numeral'), ('pillars', 'NOUN', 'noun'), ('of', 'ADP', 'adposition'), ('modern', 'ADJ', 'adjective'), ('physics', 'NOUN', 'noun'), ('.', 'PUNCT', 'punctuation')]\n",
      "Named Entities: [('Albert Einstein', 'PERSON'), ('Germany', 'GPE'), ('one', 'CARDINAL'), ('two', 'CARDINAL')]\n"
     ]
    }
   ],
   "source": [
    "# All Techniques Combined\n",
    "\n",
    "text_wrap_up = \"Albert Einstein, a theoretical physicist, was born in Germany. He developed the theory of relativity, which is one of the two pillars of modern physics.\"\n",
    "\n",
    "def full_nlp_pipeline(text):\n",
    "    doc = nlp(text)\n",
    "    segmented_sentences = [sent.text for sent in doc.sents]\n",
    "    tokens = [token.text for token in doc]\n",
    "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    pos_tags = [(token.text, token.pos_, spacy.explain(token.pos_)) for token in doc]\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return {\n",
    "        \"Sentences\": segmented_sentences,\n",
    "        \"Tokens\": tokens,\n",
    "        \"Without Stop Words\": filtered_tokens,\n",
    "        \"Lemmas\": lemmas,\n",
    "        \"POS Tags\": pos_tags,\n",
    "        \"Named Entities\": named_entities\n",
    "    }\n",
    "\n",
    "result = full_nlp_pipeline(text_wrap_up)\n",
    "\n",
    "print(\"\\nFull NLP Pipeline Output:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db820cf-9a38-4889-8656-8e8a3cb8c389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

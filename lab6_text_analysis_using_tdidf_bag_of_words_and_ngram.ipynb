{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 1: Set Up***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# corpus \n",
    "documents = [ \n",
    "    \"Data science is an interdisciplinary field.\", \n",
    "    \"Machine learning is a subset of artificial intelligence.\", \n",
    "    \"Data science uses machine learning algorithms.\", \n",
    "    \"Artificial intelligence and data science are growing fields.\" \n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 2: Bag of Words (BoW)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model:\n",
      "    algorithms  an  and  are  artificial  data  field  fields  growing  \\\n",
      "0           0   1    0    0           0     1      1       0        0   \n",
      "1           0   0    0    0           1     0      0       0        0   \n",
      "2           1   0    0    0           0     1      0       0        0   \n",
      "3           0   0    1    1           1     1      0       1        1   \n",
      "\n",
      "   intelligence  interdisciplinary  is  learning  machine  of  science  \\\n",
      "0             0                  1   1         0        0   0        1   \n",
      "1             1                  0   1         1        1   1        0   \n",
      "2             0                  0   0         1        1   0        1   \n",
      "3             1                  0   0         0        0   0        1   \n",
      "\n",
      "   subset  uses  \n",
      "0       0     0  \n",
      "1       1     0  \n",
      "2       0     1  \n",
      "3       0     0  \n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus \n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result into a DataFrame for readability \n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()) \n",
    "\n",
    "print(\"Bag of Words Model:\\n\", bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('science',\n",
       " science              3\n",
       " data                 3\n",
       " learning             2\n",
       " machine              2\n",
       " intelligence         2\n",
       " artificial           2\n",
       " is                   2\n",
       " algorithms           1\n",
       " an                   1\n",
       " are                  1\n",
       " growing              1\n",
       " fields               1\n",
       " field                1\n",
       " and                  1\n",
       " interdisciplinary    1\n",
       " of                   1\n",
       " subset               1\n",
       " uses                 1\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Which word appears most frequently across all documents? \n",
    "word_frequencies = bow_df.sum().sort_values(ascending=False)\n",
    "\n",
    "# Display most frequent word\n",
    "most_frequent_word = word_frequencies.idxmax()\n",
    "most_frequent_word, word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that appear only once: ['algorithms', 'an', 'are', 'growing', 'fields', 'field', 'and', 'interdisciplinary', 'of', 'subset', 'uses']\n"
     ]
    }
   ],
   "source": [
    "# 2. Are there any words that appear only once?\n",
    "unique_words = word_frequencies[word_frequencies == 1].index.tolist()\n",
    "print(\"Words that appear only once:\", unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 3: TF-IDF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithms</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>artificial</th>\n",
       "      <th>data</th>\n",
       "      <th>field</th>\n",
       "      <th>fields</th>\n",
       "      <th>growing</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>interdisciplinary</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>science</th>\n",
       "      <th>subset</th>\n",
       "      <th>uses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303040</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.374315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348842</td>\n",
       "      <td>0.348842</td>\n",
       "      <td>0.348842</td>\n",
       "      <td>0.442462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442462</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.496414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391378</td>\n",
       "      <td>0.391378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406289</td>\n",
       "      <td>0.406289</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.259329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406289</td>\n",
       "      <td>0.406289</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithms        an       and       are  artificial      data     field  \\\n",
       "0    0.000000  0.474771  0.000000  0.000000    0.000000  0.303040  0.474771   \n",
       "1    0.000000  0.000000  0.000000  0.000000    0.348842  0.000000  0.000000   \n",
       "2    0.496414  0.000000  0.000000  0.000000    0.000000  0.316854  0.000000   \n",
       "3    0.000000  0.000000  0.406289  0.406289    0.320323  0.259329  0.000000   \n",
       "\n",
       "     fields   growing  intelligence  interdisciplinary        is  learning  \\\n",
       "0  0.000000  0.000000      0.000000           0.474771  0.374315  0.000000   \n",
       "1  0.000000  0.000000      0.348842           0.000000  0.348842  0.348842   \n",
       "2  0.000000  0.000000      0.000000           0.000000  0.000000  0.391378   \n",
       "3  0.406289  0.406289      0.320323           0.000000  0.000000  0.000000   \n",
       "\n",
       "    machine        of   science    subset      uses  \n",
       "0  0.000000  0.000000  0.303040  0.000000  0.000000  \n",
       "1  0.348842  0.442462  0.000000  0.442462  0.000000  \n",
       "2  0.391378  0.000000  0.316854  0.000000  0.496414  \n",
       "3  0.000000  0.000000  0.259329  0.000000  0.000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display TF-IDF DataFrame\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest TF-IDF term in the first document is 'an' with a score of 0.4748\n"
     ]
    }
   ],
   "source": [
    "# 3. Which term has the highest TF-IDF score in the first document? \n",
    "highest_tfidf_term = tfidf_df.iloc[0].idxmax()\n",
    "highest_tfidf_score = tfidf_df.iloc[0].max()\n",
    "\n",
    "print(f\"The highest TF-IDF term in the first document is '{highest_tfidf_term}' with a score of {highest_tfidf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Why do some terms have a TF-IDF score of 0 in certain documents? \n",
    "# Term Absence: The term does not appear in the document, so its TF (Term Frequency) is 0, leading to a TF-IDF score of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 3: N-Grams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams (Bigrams) Model:\n",
      "    an interdisciplinary  and data  are growing  artificial intelligence  \\\n",
      "0                     1         0            0                        0   \n",
      "1                     0         0            0                        1   \n",
      "2                     0         0            0                        0   \n",
      "3                     0         1            1                        1   \n",
      "\n",
      "   data science  growing fields  intelligence and  interdisciplinary field  \\\n",
      "0             1               0                 0                        1   \n",
      "1             0               0                 0                        0   \n",
      "2             1               0                 0                        0   \n",
      "3             1               1                 1                        0   \n",
      "\n",
      "   is an  is subset  learning algorithms  learning is  machine learning  \\\n",
      "0      1          0                    0            0                 0   \n",
      "1      0          1                    0            1                 1   \n",
      "2      0          0                    1            0                 1   \n",
      "3      0          0                    0            0                 0   \n",
      "\n",
      "   of artificial  science are  science is  science uses  subset of  \\\n",
      "0              0            0           1             0          0   \n",
      "1              1            0           0             0          1   \n",
      "2              0            0           0             1          0   \n",
      "3              0            1           0             0          0   \n",
      "\n",
      "   uses machine  \n",
      "0             0  \n",
      "1             0  \n",
      "2             1  \n",
      "3             0  \n"
     ]
    }
   ],
   "source": [
    "# Using CountVectorizer for N-Grams\n",
    "# Initialize CountVectorizer with bigrams (ngram_range=(2, 2)) \n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus \n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result into a DataFrame for readability \n",
    "ngram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()) \n",
    " \n",
    "print(\"N-Grams (Bigrams) Model:\\n\", ngram_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent bigram across all documents is data science.\n"
     ]
    }
   ],
   "source": [
    "# 5. Which bigram is most frequent across all documents?\n",
    "\n",
    "bigram_counts = ngram_df.sum().sort_values(ascending=False)\n",
    "most_frequent_bigram = bigram_counts.idxmax()\n",
    "\n",
    "print(f\"Most frequent bigram across all documents is {most_frequent_bigram}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How might bigrams provide additional context compared to unigrams?\n",
    "# Bigrams help capture word relationships compared to unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Grams (Trigrams) Model:\n",
      "    an interdisciplinary field  and data science  are growing fields  \\\n",
      "0                           1                 0                   0   \n",
      "1                           0                 0                   0   \n",
      "2                           0                 0                   0   \n",
      "3                           0                 1                   1   \n",
      "\n",
      "   artificial intelligence and  data science are  data science is  \\\n",
      "0                            0                 0                1   \n",
      "1                            0                 0                0   \n",
      "2                            0                 0                0   \n",
      "3                            1                 1                0   \n",
      "\n",
      "   data science uses  intelligence and data  is an interdisciplinary  \\\n",
      "0                  0                      0                        1   \n",
      "1                  0                      0                        0   \n",
      "2                  1                      0                        0   \n",
      "3                  0                      1                        0   \n",
      "\n",
      "   is subset of  learning is subset  machine learning algorithms  \\\n",
      "0             0                   0                            0   \n",
      "1             1                   1                            0   \n",
      "2             0                   0                            1   \n",
      "3             0                   0                            0   \n",
      "\n",
      "   machine learning is  of artificial intelligence  science are growing  \\\n",
      "0                    0                           0                    0   \n",
      "1                    1                           1                    0   \n",
      "2                    0                           0                    0   \n",
      "3                    0                           0                    1   \n",
      "\n",
      "   science is an  science uses machine  subset of artificial  \\\n",
      "0              1                     0                     0   \n",
      "1              0                     0                     1   \n",
      "2              0                     1                     0   \n",
      "3              0                     0                     0   \n",
      "\n",
      "   uses machine learning  \n",
      "0                      0  \n",
      "1                      0  \n",
      "2                      1  \n",
      "3                      0  \n"
     ]
    }
   ],
   "source": [
    "#  Generating Trigrams\n",
    "# Initialize CountVectorizer with trigrams (ngram_range=(3, 3)) \n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3)) \n",
    " \n",
    "# Fit and transform the corpus \n",
    "X = vectorizer.fit_transform(documents) \n",
    " \n",
    "# Convert the result into a DataFrame for readability \n",
    "ngram_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()) \n",
    " \n",
    "print(\"N-Grams (Trigrams) Model:\\n\", ngram_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step 4: Analyze Combined Representations***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **How does TF-IDF handle frequent terms differently from BoW?**\n",
    "   - TF-IDF reduces the weight of common terms that appear in multiple documents.\n",
    "\n",
    "8. **Why might TF-IDF be preferred for some applications?**\n",
    "   - It helps in filtering out less important words and gives higher importance to distinguishing terms.\n",
    "\n",
    "9. **How do bigrams capture relationships between words?**\n",
    "   - Unlike unigrams, bigrams provide context by grouping adjacent words.\n",
    "\n",
    "10. **Provide an example where a bigram adds more meaning than individual words.**\n",
    "   - 'Machine learning' as a bigram has more meaning than 'Machine' and 'Learning' separately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

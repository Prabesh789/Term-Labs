{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Sentiment Analysis using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyzer\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What do the sentiment scores (positive, neutral, negative, and compound) represent?\n",
    "\n",
    "VADER provides four sentiment scores for a sentence:\n",
    "\n",
    "positive: The proportion of the text that has positive sentiment.\n",
    "\n",
    "neutral: The proportion of the text that is neutral.\n",
    "\n",
    "negative: The proportion of the text that has negative sentiment.\n",
    "\n",
    "compound: A normalized score that summarizes the overall sentiment, ranging from -1 (most negative) to +1 (most positive). It’s computed using a weighted sum of all the lexicon scores in the sentence, adjusted according to the rules VADER applies (like punctuation, capitalization, intensifiers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How can you classify a sentence as positive, negative, or neutral based on the compound score?\n",
    "\n",
    "We Use the following thresholds (recommended by VADER's authors):\n",
    "\n",
    "Positive: compound >= 0.05\n",
    "\n",
    "Neutral: -0.05 < compound < 0.05\n",
    "\n",
    "Negative: compound <= -0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences for VADER\n",
    "vader_sentences = [\n",
    "    \"I am so happy with the service.\",\n",
    "    \"This movie was a waste of time.\",\n",
    "    \"It was an okay experience.\",\n",
    "    \"Best purchase I've made in years!\",\n",
    "    \"I don't like this app, it's too slow.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VADER Sentiment Analysis ---\n",
      "Sentence: I am so happy with the service.\n",
      "Scores: {'neg': 0.0, 'neu': 0.559, 'pos': 0.441, 'compound': 0.6948}\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Sentence: This movie was a waste of time.\n",
      "Scores: {'neg': 0.318, 'neu': 0.682, 'pos': 0.0, 'compound': -0.4215}\n",
      "Predicted Sentiment: Negative\n",
      "\n",
      "Sentence: It was an okay experience.\n",
      "Scores: {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Sentence: Best purchase I've made in years!\n",
      "Scores: {'neg': 0.0, 'neu': 0.527, 'pos': 0.473, 'compound': 0.6696}\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Sentence: I don't like this app, it's too slow.\n",
      "Scores: {'neg': 0.232, 'neu': 0.768, 'pos': 0.0, 'compound': -0.2755}\n",
      "Predicted Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- VADER Sentiment Analysis ---\")\n",
    "for sentence in vader_sentences:\n",
    "    scores = vader_analyzer.polarity_scores(sentence)\n",
    "    compound = scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    print(f\"Sentence: {sentence}\\nScores: {scores}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sentiment Analysis using Huggingface Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the labels provided by the Huggingface model for sentiment analysis?\n",
    "By default, the Huggingface sentiment analysis pipeline provides two labels:\n",
    "\n",
    "POSITIVE\n",
    "\n",
    "NEGATIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How do the confidence scores relate to the model's prediction?\n",
    "The confidence score represents the model's probability (softmax output) that the prediction is correct. It ranges between 0 and 1.\n",
    "\n",
    "A higher score (e.g., 0.98) means the model is very confident in its prediction.\n",
    "\n",
    "A lower score (e.g., 0.55) indicates uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raipr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\raipr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\raipr\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "hf_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences for Huggingface\n",
    "hf_sentences = [\n",
    "    \"I love this new phone.\",\n",
    "    \"I had a terrible experience with customer support.\",\n",
    "    \"The movie was not bad, but not great either.\",\n",
    "    \"Absolutely loved the restaurant!\",\n",
    "    \"The product arrived damaged, very disappointed.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Huggingface Sentiment Analysis ---\n",
      "Sentence: I love this new phone.\n",
      "Label: POSITIVE, Score: 0.9998\n",
      "\n",
      "Sentence: I had a terrible experience with customer support.\n",
      "Label: NEGATIVE, Score: 0.9995\n",
      "\n",
      "Sentence: The movie was not bad, but not great either.\n",
      "Label: NEGATIVE, Score: 0.9963\n",
      "\n",
      "Sentence: Absolutely loved the restaurant!\n",
      "Label: POSITIVE, Score: 0.9999\n",
      "\n",
      "Sentence: The product arrived damaged, very disappointed.\n",
      "Label: NEGATIVE, Score: 0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Huggingface Sentiment Analysis ---\")\n",
    "for sentence in hf_sentences:\n",
    "    result = hf_pipeline(sentence)[0]\n",
    "    print(f\"Sentence: {sentence}\\nLabel: {result['label']}, Score: {result['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare VADER and Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparison of VADER and Huggingface ---\n",
      "Sentence: I love this new phone.\n",
      "VADER: Sentiment=Positive, Compound=0.6369\n",
      "Huggingface: Sentiment=POSITIVE, Score=0.9998\n",
      "\n",
      "Sentence: I had a terrible experience with customer support.\n",
      "VADER: Sentiment=Negative, Compound=-0.1027\n",
      "Huggingface: Sentiment=NEGATIVE, Score=0.9995\n",
      "\n",
      "Sentence: The movie was not bad, but not great either.\n",
      "VADER: Sentiment=Negative, Compound=-0.5448\n",
      "Huggingface: Sentiment=NEGATIVE, Score=0.9963\n",
      "\n",
      "Sentence: Absolutely loved the restaurant!\n",
      "VADER: Sentiment=Positive, Compound=0.6689\n",
      "Huggingface: Sentiment=POSITIVE, Score=0.9999\n",
      "\n",
      "Sentence: The product arrived damaged, very disappointed.\n",
      "VADER: Sentiment=Negative, Compound=-0.7425\n",
      "Huggingface: Sentiment=NEGATIVE, Score=0.9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combined test sentences\n",
    "combined_sentences = hf_sentences\n",
    "\n",
    "print(\"--- Comparison of VADER and Huggingface ---\")\n",
    "for sentence in combined_sentences:\n",
    "    vader_score = vader_analyzer.polarity_scores(sentence)\n",
    "    hf_result = hf_pipeline(sentence)[0]\n",
    "\n",
    "    # Determine VADER sentiment\n",
    "    compound = vader_score['compound']\n",
    "    if compound >= 0.05:\n",
    "        vader_sentiment = 'Positive'\n",
    "    elif compound <= -0.05:\n",
    "        vader_sentiment = 'Negative'\n",
    "    else:\n",
    "        vader_sentiment = 'Neutral'\n",
    "\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"VADER: Sentiment={vader_sentiment}, Compound={compound}\")\n",
    "    print(f\"Huggingface: Sentiment={hf_result['label']}, Score={hf_result['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do the results of VADER and Huggingface compare in terms of sentiment classification?\n",
    "VADER is rule-based and uses a lexicon of words with pre-assigned sentiment values. It works well for simple and clear-cut sentiment in texts like reviews, tweets, etc.\n",
    "\n",
    "Huggingface captures context and semantics much better. It’s more accurate on nuanced language, like “The movie wasn’t bad” (which VADER might misclassify as negative because of the word \"bad\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which method provides a more accurate prediction for complex sentences (e.g., sentences with sarcasm)?\n",
    "Huggingface provides better predictions for complex sentences, sarcasm, or double negatives because it’s context-aware.\n",
    "\n",
    "VADER struggles with sarcasm since it doesn't “understand” context, it just adds up word scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which method is faster? Why might that be the case?\n",
    "VADER is faster as it’s lightweight and purely rule-based, with no model loading or GPU acceleration required.\n",
    "\n",
    "Huggingface is slower because:\n",
    "\n",
    "It loads a large pre-trained transformer model.\n",
    "\n",
    "It runs each sentence through deep neural networks.\n",
    "\n",
    "It might require hardware acceleration (GPU/TPU) for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluating Sentiment Analysis Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Test Dataset\n",
    "test_data = pd.DataFrame({\n",
    "    'Sentence': [\n",
    "        \"I really enjoyed the movie.\",\n",
    "        \"The service was terrible.\",\n",
    "        \"It was just fine, nothing special.\",\n",
    "        \"I absolutely hated the product.\",\n",
    "        \"This is the best experience ever.\"\n",
    "    ],\n",
    "    'True Sentiment': ['positive', 'negative', 'neutral', 'negative', 'positive']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform Sentiment Analysis\n",
    "vader_preds = []\n",
    "hf_preds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_data['Sentence']:\n",
    "    # VADER prediction\n",
    "    compound = vader_analyzer.polarity_scores(sentence)['compound']\n",
    "    if compound >= 0.05:\n",
    "        vader_preds.append('positive')\n",
    "    elif compound <= -0.05:\n",
    "        vader_preds.append('negative')\n",
    "    else:\n",
    "        vader_preds.append('neutral')\n",
    "\n",
    "    # Huggingface prediction\n",
    "    label = hf_pipeline(sentence)[0]['label'].lower()\n",
    "    if label == 'positive':\n",
    "        hf_preds.append('positive')\n",
    "    else:\n",
    "        hf_preds.append('negative')  # Huggingface usually returns POSITIVE or NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "VADER - Accuracy: 0.80, Precision: 0.67, Recall: 0.80, F1: 0.72\n",
      "Huggingface - Accuracy: 0.80, Precision: 0.67, Recall: 0.80, F1: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Calculate Evaluation Metrics\n",
    "vader_metrics = precision_recall_fscore_support(test_data['True Sentiment'], vader_preds, average='weighted', zero_division=0)\n",
    "hf_metrics = precision_recall_fscore_support(test_data['True Sentiment'], hf_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(f\"VADER - Accuracy: {accuracy_score(test_data['True Sentiment'], vader_preds):.2f}, Precision: {vader_metrics[0]:.2f}, Recall: {vader_metrics[1]:.2f}, F1: {vader_metrics[2]:.2f}\")\n",
    "print(f\"Huggingface - Accuracy: {accuracy_score(test_data['True Sentiment'], hf_preds):.2f}, Precision: {hf_metrics[0]:.2f}, Recall: {hf_metrics[1]:.2f}, F1: {hf_metrics[2]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do the models perform in terms of accuracy, precision, recall, and F1-score?\n",
    "VADER generally performs well on simple, short texts, especially where sentiment is explicit.\n",
    "\n",
    "Huggingface tends to achieve higher accuracy, precision, recall, and F1-score, especially on complex or subtle text, due to its understanding of language context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which model performs better in predicting positive sentiment? Negative sentiment?\n",
    "Positive sentiment: Both models perform well, but Huggingface tends to be more confident and consistent in predictions.\n",
    "\n",
    "Negative sentiment: Huggingface usually performs better here. VADER may misclassify subtle or indirect negativity (e.g., “not worth the price”) because it lacks contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What might cause discrepancies between the two models' predictions?\n",
    "Several factors can lead to differences:\n",
    "\n",
    "Contextual Understanding: Huggingface understands grammar, negation, and context; VADER doesn’t.\n",
    "\n",
    "E.g., “I’m not happy” → VADER might pick up “happy” as a positive word.\n",
    "\n",
    "Model Type: VADER uses a fixed wordlist and rules; Huggingface uses a learned model from millions of text samples.\n",
    "\n",
    "Sentence Length and Complexity: VADER performs better on short, clear statements. Huggingface handles long and nuanced sentences better.\n",
    "\n",
    "Sarcasm and Slang: Huggingface can detect sarcasm/slang better (to an extent), whereas VADER might misinterpret them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
